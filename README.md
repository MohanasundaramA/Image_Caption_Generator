# Image Caption Generator

## Abstract
Image caption generator is a process of recognizing the context of an image and annotating it with relevant captions using deep learning, and computer vision. It includes the labeling of an image with English keywords with the help of datasets provided during model training. Imagenet dataset is used to train the CNN model called VGG16 and Xception. These two models are used for image feature extraction. These extracted features will be fed to the LSTM model which in turn generates the image caption.

## Architecture
The model architecture consists of a CNN which extracts the features and encodes the input image and a Recurrent Neural Network (RNN) based on Long Short Term Memory (LSTM) layers. The most significant difference with other models is that the image embedding is provided as the first input to the RNN network and only once.

<img width="1430" alt="decoder" src="https://user-images.githubusercontent.com/117024021/211224924-f84e80e8-d8d3-4c07-be1a-0e868f7b2883.png">

## Required libraries
- Python - 3.6.7
+ Numpy - 1.16.4
* Tensorflow - 1.13.1
- Keras - 2.2.4
+ nltk - 3.2.5
* PIL - 4.3.0
- Matplotlib - 3.0.3

## Dataset
The Flickr_8K dataset is used for the model training of image caption generators. The dataset can be downloaded directly from the below links. This dataset includes around 8000 images along with 5 different captions written by different people for each image. The most important file is Flickr 8k.token which is storing all the image names with the captions respectively. 8091 images are stored inside the Flicker8k_Dataset folder and the text files with captions of images are stored in the Flickr_8k_text folder.

- [Flicker8k_Dataset](https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip)
+ [Flickr_8k_text](https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip)

## Flow of the project
a. Extracting features from images using VGG-16 and Xception
b. Cleaning the caption data
c. Merging the captions and images
d. Building LSTM model for training
e. Predicting on test data
f. Evaluating the captions using BLEU scores as the metric

## Model
![model](https://user-images.githubusercontent.com/117024021/211224971-6cfa9244-76db-4a01-80ae-f36351f5b83d.png)

## Results
- BLEU-1:  53%
+ BLEU-2:  28%
* BLEU-3:  19%
- BLEU-4:  9%

Here are some captions generated by this model:

![output](https://user-images.githubusercontent.com/117024021/211225142-be039443-0b4f-48f3-aed3-f03eda121ef2.jpg)

## Applications
- Some detailed usecases would be like an visually impaired person taking a picture from his phone and then the caption generator will turn the caption to speech for him to understand.
+ Advertising industry trying the generate captions automatically without the need to make them seperately during production and sales.
* Doctors can use this technology to find tumors or some defects in the images or used by people for understanding geospatial images where they can find out more details about the terrain.

## Conclusion
Implementing the model is a time consuming task as it involved lot of testing with different hyperparameters to generate better captions. The model generates good captions for the provided image but it can always be improved.
